import unittest
import json
import spacy

nlp = spacy.load("en_core_web_sm")

# sys path manipulation necessary for importing function defined in parent dir
import os, sys
sys.path.insert( 0, os.getcwd() ) 
from extract_words import (
    analyze_file,
    analyze_file_sentence_ids,
    separate_fpath,
    get_doc_word_stats
)

class TestAnalyzeFile( unittest.TestCase ):

    def _test_similarity_for_file( self, fname ):
        wordcount_path = "data/" + separate_fpath( fname )[ 1 ] + ".json"
        wsid_path = "data/" + separate_fpath( fname )[ 1 ] + "_wsid.json"
        keep_generated_wordcounts = os.path.isfile( wordcount_path )
        keep_generated_wsid = os.path.isfile( wsid_path )

        analyze_file( "data/" +  fname, "data/" )
        analyze_file_sentence_ids( "data/" +  fname, "data/" )
        
        wordcounts = None
        wsid = None
        with open( wordcount_path, "r" ) as f:
            wordcounts = json.load( f )
        with open( wsid_path, "r" ) as f:
            wsid = json.load( f )

        # compute the differences of the two sets of words
        in_first = set( wordcounts.keys() ) - set( wsid.keys() )
        in_second = set( wsid.keys() ) - set( wordcounts.keys() )

        # because of the difference in context size, the two analyzers may sometimes
        # lemmatize the same word differently;
        #
        # clean up any words that actually occur in both sets, but were classified
        # as different because of the differences in lemmatization
        # e.g. "whisper"<->"whispers", "child"<->"children"
        removable = set()
        for word in in_first:
            token = nlp( word )[ 0 ]
            if ( ( token.text in wsid ) or
                 ( token.lemma_ in wsid ) ):
                removable.add( token.text )
                removable.add( token.lemma_ )

        for word in in_second:
            token = nlp( word )[ 0 ]
            if ( ( token.lemma_ in wordcounts ) or
                 ( token.text in wordcounts ) ):
                removable.add( token.text )
                removable.add( token.lemma_ )

        in_first = in_first - removable
        in_second = in_second - removable

        in_first_string = "[" + ", ".join( in_first ) + "]"
        print( "Words only identified by first analyzer: " )
        print( in_first_string )
        print()

        in_second_string = "[" + ", ".join( in_second ) + "]"
        print( "Words only identified by second analyzer: " )
        print( in_second_string )
        print()

        # some of the differences in lemmatization cannot be overcome by the above
        # method of analyzing each word individually, e.g. the second analyzer
        # registers the word "singing" as an instance of the verb "singe", as
        # opposed to the contextually correct "sing"
        #
        # in addition, the first analyzer is likely to pick up srt format artifacts
        # such as "subs.com</font", "color="#fffa00" as words
        #
        # thus the test tolerates a small number of words unique to each analyzer
        # output as long as it is less than 1% of the total unique words identified
        # by the analyzer
        msg = "The words only identified by the first analyzer are >= 1% of the "\
              "total unique words identified by the first analyzer"
        self.assertLess( len( in_first ) / len( wordcounts ), 0.01, msg )

        msg = "The words only identified by the second analyzer are >= 1% of the "\
              "total unique words identified by the second analyzer"
        self.assertLess( len( in_second) / len( wsid ), 0.01, msg )


        # clean up autogenerated files if they didn't exist before test
        if not keep_generated_wordcounts:
            os.unlink( wordcount_path )
        if not keep_generated_wsid:
            os.unlink( wsid_path )

    def test_similarity( self ):
        """
        test that analyze_file and analyze_file_sentence_ids arrive at the same
        number of occurrences in the doc for each word
        """
        self._test_similarity_for_file( "its-a-wonderful-life-1946.srt" )
        self._test_similarity_for_file( "penny-serenade-1941.srt" )

    def test_tf_idf_similarity( self ):
        """
        basic test that checks the parity for the top 120 (sorted by tf-idf) results
        for the 2 analyzers, based on the example "detour-1945.srt". The test also
        checks that the full sets of results are not entirely identical (this is a
        sanity check that two different analyzers are used).

        Note: this test is not guaranteed to work for other .srt files, because in
        others, the small difference may occur in some of the top 120 words, which
        will lead to the first assertion to fail.

        The purpose of the test is not to ensure complete parity given any .srt
        file, but rather to make sure that the workflow using the new analyzer does
        not suffer from major bugs (which would be visible as significant
        differences in the sets).
        """
        wordcount_stats = get_doc_word_stats( "data/", "detour-1945", False )
        wsid_stats = get_doc_word_stats( "data/", "detour-1945", True )

        # delete the data for word occurrence ids, as this information is only
        # meaningful in the wsid_stats collection
        # (first element is None in both, skip it)
        for i in range( len( wsid_stats ) ):
            if i == 0:
                continue

            del wordcount_stats[ i ][ 1 ][ "word_occ_ids" ]
            del wsid_stats[ i ][ 1 ][ "word_occ_ids" ]

        # check that results for the top 120 words are identical for the 2 analyzers
        self.assertCountEqual( wordcount_stats[ :120 ], wsid_stats[ :120 ] )

        # sanity check: the full stats for the 2 analyzers for this doc are slightly
        # different, because of minor differences in how spacy.nlp lemmatizes
        # certain words depending on the context
        self.assertNotEqual( wordcount_stats, wsid_stats )

if __name__ == '__main__':
    unittest.main()
